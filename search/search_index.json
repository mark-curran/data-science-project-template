{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Project Template Welcome the documentation site for the Data Science Project Template, a codebase that builds containers appropriate for data science projects. While this template provides structure for a Data Science codebase, it is not meant to be prescriptive. For those looking to get up and running quickly , the template has ready integration with docker, vscode and jupyterlab. For those looking to create a reusable code structure in a specific enterprise setting, it might be worthwhile reading the design decisions behind the template. Forking this repo should be a starting point, not the final destination. Project Status This project is being actively built, and though most of the core functionality is complete the documentation is not in a state where the template can be readily used. Further Information Quickstart and Installation The Quickstart section section describes how to initialize the template and setup a local development environment. Template Design Information on the template design can be found in the Design section . This section describes the design decisions behind the template, and the structure of the code in more detail. Additional Features The Additional Features section describes how the various features of the template can be used in different scenarios. Further Reading Additional information and links to the technologies used in the project can be found in the Further Reading section . Distribution See the About section for further information on distribution and reuse.","title":"Home"},{"location":"#data-science-project-template","text":"Welcome the documentation site for the Data Science Project Template, a codebase that builds containers appropriate for data science projects. While this template provides structure for a Data Science codebase, it is not meant to be prescriptive. For those looking to get up and running quickly , the template has ready integration with docker, vscode and jupyterlab. For those looking to create a reusable code structure in a specific enterprise setting, it might be worthwhile reading the design decisions behind the template. Forking this repo should be a starting point, not the final destination.","title":"Data Science Project Template"},{"location":"#project-status","text":"This project is being actively built, and though most of the core functionality is complete the documentation is not in a state where the template can be readily used.","title":"Project Status"},{"location":"#further-information","text":"","title":"Further Information"},{"location":"#quickstart-and-installation","text":"The Quickstart section section describes how to initialize the template and setup a local development environment.","title":"Quickstart and Installation"},{"location":"#template-design","text":"Information on the template design can be found in the Design section . This section describes the design decisions behind the template, and the structure of the code in more detail.","title":"Template Design"},{"location":"#additional-features","text":"The Additional Features section describes how the various features of the template can be used in different scenarios.","title":"Additional Features"},{"location":"#further-reading","text":"Additional information and links to the technologies used in the project can be found in the Further Reading section .","title":"Further Reading"},{"location":"#distribution","text":"See the About section for further information on distribution and reuse.","title":"Distribution"},{"location":"about/about/","text":"About This page contains information about the Data Science Project Template. Author This project was started by Mark Curran , and any issues can be submitted using the github issues system. License This project can be distributed under GNU GENERAL PUBLIC LICENSE Version 3. A link to the full license can be found here .","title":"About"},{"location":"about/about/#about","text":"This page contains information about the Data Science Project Template.","title":"About"},{"location":"about/about/#author","text":"This project was started by Mark Curran , and any issues can be submitted using the github issues system.","title":"Author"},{"location":"about/about/#license","text":"This project can be distributed under GNU GENERAL PUBLIC LICENSE Version 3. A link to the full license can be found here .","title":"License"},{"location":"additional_features/accessing_credentials/","text":"Accessing Credentials This page explains how to access credentials inside a container. Git credentials in local environment Git credentials in local environment using ssh-agent or the vscode remote extensions package. Other Services How to get GCP, AWS or Azure credentials inside a container.","title":"Accessing Credentials"},{"location":"additional_features/accessing_credentials/#accessing-credentials","text":"This page explains how to access credentials inside a container.","title":"Accessing Credentials"},{"location":"additional_features/accessing_credentials/#git-credentials-in-local-environment","text":"Git credentials in local environment using ssh-agent or the vscode remote extensions package.","title":"Git credentials in local environment"},{"location":"additional_features/accessing_credentials/#other-services","text":"How to get GCP, AWS or Azure credentials inside a container.","title":"Other Services"},{"location":"additional_features/editing_requirements/","text":"Editing the requirements file The requirements file contains a complete list of all python packages needed for an environment to run. If you would like to add a package to an environment, first install the package using the command pip install <package_name> This will add the package to the evironment's python installation. One can also remove packages using the pip uninstall command. To automatically install these packages next time the container is built, you will need to update the requirements file. To produce the updated package list in the format required, run the command pip freeze and paste its output into the requirements file.","title":"Editing the requirements file"},{"location":"additional_features/editing_requirements/#editing-the-requirements-file","text":"The requirements file contains a complete list of all python packages needed for an environment to run. If you would like to add a package to an environment, first install the package using the command pip install <package_name> This will add the package to the evironment's python installation. One can also remove packages using the pip uninstall command. To automatically install these packages next time the container is built, you will need to update the requirements file. To produce the updated package list in the format required, run the command pip freeze and paste its output into the requirements file.","title":"Editing the requirements file"},{"location":"additional_features/jupyterlab/","text":"Jupyterlab Jupyterlab is a web-based development environment that is particularly useful for data manipulation and visualization in python. This page explains how to setup Juypyterlab, i.e., the jupyterlab server, in the dev environment. It is strongly recommended to read the section on port mapping below before starting the server. Starting the dev container Start the dev service on the machine that will host the jupyter server docker-compose run -it --service-ports dev /bin/bash Jupyterlab creates notebook files, and it is convenient to run the command to start the server from the folder where you want to store these files. For example, to store your notebooks in dev/notebooks , run this command cd /workspaces/dev/notebooks Note that the root of the repository is /workspaces inside the dev environment. From here start the server by running jupyter lab --port=8001 --ip 0.0.0.0 --no-browser or if you are running as root inside the container jupyter lab --port=8001 --ip 0.0.0.0 --no-browser --allow-root A message should print to the terminal with a URL from which can access the server, for example http://127.0.0.1:8001/lab?token= followed by a long string of characters. These characters are the token that permits access to the server, and by extension the host machine. Do not share the token with people who should not have access to the host machine. You can detach from the container by pressing the detach keys sequence for your system (the default is CTRL-p CTRL-q ). Port Mapping By starting the jupyter lab server, you give any user direct access to the host. Therefore, it is important to consider who can access the host's ip address before starting the server. You can specify the host ip address and the port to expose, as explained in the docker-compose specification . Note, that by default docker-compose will not expose ports when running a container , hence when ports are specified in the compose file, then the argument --service-ports must also be passed. Managing extensions and configs It is possible to standardize the behavior of Jupyterlab across a wider team by mounting jupyter configs on the host machine. The jupyter documentation lists common directories and file locations that one could leverage.","title":"Jupyterlab"},{"location":"additional_features/jupyterlab/#jupyterlab","text":"Jupyterlab is a web-based development environment that is particularly useful for data manipulation and visualization in python. This page explains how to setup Juypyterlab, i.e., the jupyterlab server, in the dev environment. It is strongly recommended to read the section on port mapping below before starting the server.","title":"Jupyterlab"},{"location":"additional_features/jupyterlab/#starting-the-dev-container","text":"Start the dev service on the machine that will host the jupyter server docker-compose run -it --service-ports dev /bin/bash Jupyterlab creates notebook files, and it is convenient to run the command to start the server from the folder where you want to store these files. For example, to store your notebooks in dev/notebooks , run this command cd /workspaces/dev/notebooks Note that the root of the repository is /workspaces inside the dev environment. From here start the server by running jupyter lab --port=8001 --ip 0.0.0.0 --no-browser or if you are running as root inside the container jupyter lab --port=8001 --ip 0.0.0.0 --no-browser --allow-root A message should print to the terminal with a URL from which can access the server, for example http://127.0.0.1:8001/lab?token= followed by a long string of characters. These characters are the token that permits access to the server, and by extension the host machine. Do not share the token with people who should not have access to the host machine. You can detach from the container by pressing the detach keys sequence for your system (the default is CTRL-p CTRL-q ).","title":"Starting the dev container"},{"location":"additional_features/jupyterlab/#port-mapping","text":"By starting the jupyter lab server, you give any user direct access to the host. Therefore, it is important to consider who can access the host's ip address before starting the server. You can specify the host ip address and the port to expose, as explained in the docker-compose specification . Note, that by default docker-compose will not expose ports when running a container , hence when ports are specified in the compose file, then the argument --service-ports must also be passed.","title":"Port Mapping"},{"location":"additional_features/jupyterlab/#managing-extensions-and-configs","text":"It is possible to standardize the behavior of Jupyterlab across a wider team by mounting jupyter configs on the host machine. The jupyter documentation lists common directories and file locations that one could leverage.","title":"Managing extensions and configs"},{"location":"additional_features/users_inside_container/","text":"Running Docker Containers as a non-root User This document explains some of the difficulties of using a container as a non-root user. This page is a work in progress. For now please refer to these articles: * https://www.howtogeek.com/devops/why-processes-in-docker-containers-shouldnt-run-as-root/ * https://code.visualstudio.com/remote/advancedcontainers/add-nonroot-user","title":"Running Docker Containers as a non-root User"},{"location":"additional_features/users_inside_container/#running-docker-containers-as-a-non-root-user","text":"This document explains some of the difficulties of using a container as a non-root user. This page is a work in progress. For now please refer to these articles: * https://www.howtogeek.com/devops/why-processes-in-docker-containers-shouldnt-run-as-root/ * https://code.visualstudio.com/remote/advancedcontainers/add-nonroot-user","title":"Running Docker Containers as a non-root User"},{"location":"additional_features/using_mkdocs/","text":"Using mkdocs Note that run mkdocs locally you need to run: 0.0.0.0:8000 and make sure that you are exposing port 8000 (or whichever port you choose) in the docker-compose file.","title":"Using mkdocs"},{"location":"additional_features/using_mkdocs/#using-mkdocs","text":"Note that run mkdocs locally you need to run: 0.0.0.0:8000 and make sure that you are exposing port 8000 (or whichever port you choose) in the docker-compose file.","title":"Using mkdocs"},{"location":"design/design/","text":"Project Design This project was designed to integrate with the environments one encounters in a typical data science project. The definitions of said environments, and whether the default settings of the template address them, is of course open to debate. That said, the template design can be used as a starting point, and is not intended as a one size fits all solution. Environments in a Data Sciene Project Though a data science project can have the same phases (data exploration, model design,...) that one finds in any explanation of a DevOps or MLOps workflow, usually there are three distinct environments one works with. Given the different purposes of these environments, it makes sense to have different configurations to address them. In this project, each environment corresponds to a service within the docker compose file . Local Development The users local machine, containing source code, git configurations, code linters and checkers. Running elsewhere on the local machine there is probably a service for managing the users personal credentials . A local machine might have network and resource constraints, which makes it inappropriate for large scale data processing. Dev Development A remote environment with access to relevant data. This environment would need access to any relevant data manipulation of ML libraries, e.g., numpy, tensorflow, etc. Packages for debugging, logging and telemetry might be required. Depending on data access permissions, it might make sense to mount the credentials of a service account on this machine. Staging/Production A remote environment identical to production. This environment might be used for testing prior to final deployment or might be production itself. Only the necessary packges are installed in this environment, and personal credentials should certainly not be used to access services. Configurations local The local config is stored in the .devcontainer folder. As per the devcontainer specification , the file .devcontainer.json configures the local environment by starting the local service in the docker compose file and by configuring any vscode extensions. By default the main branch of the repository contains a number of vscode extensions that are relevant to local development. An uncompromising python code formatter . A SQL formatter that works with multiple dialects, in particular, multiple cloud providers. The default requirements file for this environment is .devcontainer/local-requirements.txt . dev The dev environment is created by running the dev service in the docker compose file . Its requirements file is dev/dev-requirements.txt . By default the main branch of the repository contains a number of packages that are relevant to dev development. Exploratory data analysis and visualizations using Jupyterlab . Data manipulation using pandas . prod The prod environment is created by running the prod service in the docker compose file . Its requirements file is prod-requirements.txt . By default the main branch of the repository contains only the packages that might be relevant in a production setting. Data manipulation using pandas .","title":"Project Design"},{"location":"design/design/#project-design","text":"This project was designed to integrate with the environments one encounters in a typical data science project. The definitions of said environments, and whether the default settings of the template address them, is of course open to debate. That said, the template design can be used as a starting point, and is not intended as a one size fits all solution.","title":"Project Design"},{"location":"design/design/#environments-in-a-data-sciene-project","text":"Though a data science project can have the same phases (data exploration, model design,...) that one finds in any explanation of a DevOps or MLOps workflow, usually there are three distinct environments one works with. Given the different purposes of these environments, it makes sense to have different configurations to address them. In this project, each environment corresponds to a service within the docker compose file .","title":"Environments in a Data Sciene Project"},{"location":"design/design/#local-development","text":"The users local machine, containing source code, git configurations, code linters and checkers. Running elsewhere on the local machine there is probably a service for managing the users personal credentials . A local machine might have network and resource constraints, which makes it inappropriate for large scale data processing.","title":"Local Development"},{"location":"design/design/#dev-development","text":"A remote environment with access to relevant data. This environment would need access to any relevant data manipulation of ML libraries, e.g., numpy, tensorflow, etc. Packages for debugging, logging and telemetry might be required. Depending on data access permissions, it might make sense to mount the credentials of a service account on this machine.","title":"Dev Development"},{"location":"design/design/#stagingproduction","text":"A remote environment identical to production. This environment might be used for testing prior to final deployment or might be production itself. Only the necessary packges are installed in this environment, and personal credentials should certainly not be used to access services.","title":"Staging/Production"},{"location":"design/design/#configurations","text":"","title":"Configurations"},{"location":"design/design/#local","text":"The local config is stored in the .devcontainer folder. As per the devcontainer specification , the file .devcontainer.json configures the local environment by starting the local service in the docker compose file and by configuring any vscode extensions. By default the main branch of the repository contains a number of vscode extensions that are relevant to local development. An uncompromising python code formatter . A SQL formatter that works with multiple dialects, in particular, multiple cloud providers. The default requirements file for this environment is .devcontainer/local-requirements.txt .","title":"local"},{"location":"design/design/#dev","text":"The dev environment is created by running the dev service in the docker compose file . Its requirements file is dev/dev-requirements.txt . By default the main branch of the repository contains a number of packages that are relevant to dev development. Exploratory data analysis and visualizations using Jupyterlab . Data manipulation using pandas .","title":"dev"},{"location":"design/design/#prod","text":"The prod environment is created by running the prod service in the docker compose file . Its requirements file is prod-requirements.txt . By default the main branch of the repository contains only the packages that might be relevant in a production setting. Data manipulation using pandas .","title":"prod"},{"location":"further_reading/further_reading/","text":"Further Reading Some further reading material.","title":"Further Reading"},{"location":"further_reading/further_reading/#further-reading","text":"Some further reading material.","title":"Further Reading"},{"location":"quickstart/quickstart/","text":"Quickstart This page describes how to set up the Data Science Project Template for local development. This project template is in effect a hierarchy of configs that build containers based on the environment that data science is being conducted in. At the bottom of that hierarchy is the local container that runs on a users local machine. This page describes creating and developing inside the local container. Required Software You will need the following applications installed on your local machine. Docker Docker is the application that will run the local container. To install Docker desktop on a Windows or Mac please to the official site . There are alternatives such as podman , though the project has not been tested on this engine. For Windows Users: Windows Subsystem for Linux If you are using a Windows machine, then you must additionally install Windows Subsystem for Linux . Instructions on how to enable WSL with Docker on a windows machine can be found in the official documentation . Optional: Visual Studio Code Once you have installed Docker, it is highly recommended to install vscode . The config for the local container is structured according to the devcontainer specification , and vscode has an extension ( Remote - Containers ) that is compatible with this spec. Once vscode is installed, install the Remote - Containers extension as per the instructions here . If you are familiar with Docker and are happy to run docker-compose from the command line, then vscode is not necessary. Optional: Installing Docker into a Linux Subsystem At the time of writing (September 2022), Docker desktop is only available as free software for personal projects and organizations below a certain size. However, the docker engine can be freely installed on any linux machine. To install the docker engine on a linux machine refer to the official documentation which explains how to install the docker engine on various distros, e.g., Ubuntu . If you are running Windows or Mac and your organisation does not have a license to use Docker desktop, you can still run the docker engine from within a virtual machine. Docker Desktop pricing . Running the docker engine in WSL . Running the docker engine on MacOS, both arm64 and x86 chips . Building a Local Development Container Persisting Command Line History The main branch of the Data Science Project Template repository contains all the files you will need to start developing locally, bar one. When developing inside the container, your bash history is kept in the file \"/root/.bash_history\" (by default the container runs as the root user, this page explains some of the limitations of this approach). To prevent your history being destroyed when you rebuild the container, this file is persisted to the file \".devcontainer/.bash_history\" on the local machine. To prevent this file being committed to a remote machine, this file is part of the \".gitignore\", and as such is not present in the main branch. If you would like to persist command line history between sessions, create an empty file \".devcontainer/.bash_history\" before proceeding to build the container for the first time. If you don't want your command line history to be preserved, then comment out this line in the docker-compose.yml file. Building and Running the Container To build and connect to the container, open vscode and open the root folder of the Data Science Project Template repo. Make sure Docker Desktop is running. If the Remote - Containers extension is installed correctly, then you should be able to build the container by bringing up the command palette and entering the command Remote-Containers: Rebuild Container . Installing vscode extensions By default the main branch of the template has configurations for a number of vscode extensions under the key extensions.vscode . Note, that these extensions (or any other the users want) need to be installed in vscode prior to starting the container. Alternatively, this key can be removed from the .devcontainer.json file if not extensions are required. Optional: Storing code inside WSL If you are using WSL then it is recommended that you further store your code in WSL 2 filesystem .","title":"Quickstart"},{"location":"quickstart/quickstart/#quickstart","text":"This page describes how to set up the Data Science Project Template for local development. This project template is in effect a hierarchy of configs that build containers based on the environment that data science is being conducted in. At the bottom of that hierarchy is the local container that runs on a users local machine. This page describes creating and developing inside the local container.","title":"Quickstart"},{"location":"quickstart/quickstart/#required-software","text":"You will need the following applications installed on your local machine.","title":"Required Software"},{"location":"quickstart/quickstart/#docker","text":"Docker is the application that will run the local container. To install Docker desktop on a Windows or Mac please to the official site . There are alternatives such as podman , though the project has not been tested on this engine.","title":"Docker"},{"location":"quickstart/quickstart/#for-windows-users-windows-subsystem-for-linux","text":"If you are using a Windows machine, then you must additionally install Windows Subsystem for Linux . Instructions on how to enable WSL with Docker on a windows machine can be found in the official documentation .","title":"For Windows Users: Windows Subsystem for Linux"},{"location":"quickstart/quickstart/#optional-visual-studio-code","text":"Once you have installed Docker, it is highly recommended to install vscode . The config for the local container is structured according to the devcontainer specification , and vscode has an extension ( Remote - Containers ) that is compatible with this spec. Once vscode is installed, install the Remote - Containers extension as per the instructions here . If you are familiar with Docker and are happy to run docker-compose from the command line, then vscode is not necessary.","title":"Optional: Visual Studio Code"},{"location":"quickstart/quickstart/#optional-installing-docker-into-a-linux-subsystem","text":"At the time of writing (September 2022), Docker desktop is only available as free software for personal projects and organizations below a certain size. However, the docker engine can be freely installed on any linux machine. To install the docker engine on a linux machine refer to the official documentation which explains how to install the docker engine on various distros, e.g., Ubuntu . If you are running Windows or Mac and your organisation does not have a license to use Docker desktop, you can still run the docker engine from within a virtual machine. Docker Desktop pricing . Running the docker engine in WSL . Running the docker engine on MacOS, both arm64 and x86 chips .","title":"Optional: Installing Docker into a Linux Subsystem"},{"location":"quickstart/quickstart/#building-a-local-development-container","text":"","title":"Building a Local Development Container"},{"location":"quickstart/quickstart/#persisting-command-line-history","text":"The main branch of the Data Science Project Template repository contains all the files you will need to start developing locally, bar one. When developing inside the container, your bash history is kept in the file \"/root/.bash_history\" (by default the container runs as the root user, this page explains some of the limitations of this approach). To prevent your history being destroyed when you rebuild the container, this file is persisted to the file \".devcontainer/.bash_history\" on the local machine. To prevent this file being committed to a remote machine, this file is part of the \".gitignore\", and as such is not present in the main branch. If you would like to persist command line history between sessions, create an empty file \".devcontainer/.bash_history\" before proceeding to build the container for the first time. If you don't want your command line history to be preserved, then comment out this line in the docker-compose.yml file.","title":"Persisting Command Line History"},{"location":"quickstart/quickstart/#building-and-running-the-container","text":"To build and connect to the container, open vscode and open the root folder of the Data Science Project Template repo. Make sure Docker Desktop is running. If the Remote - Containers extension is installed correctly, then you should be able to build the container by bringing up the command palette and entering the command Remote-Containers: Rebuild Container .","title":"Building and Running the Container"},{"location":"quickstart/quickstart/#installing-vscode-extensions","text":"By default the main branch of the template has configurations for a number of vscode extensions under the key extensions.vscode . Note, that these extensions (or any other the users want) need to be installed in vscode prior to starting the container. Alternatively, this key can be removed from the .devcontainer.json file if not extensions are required.","title":"Installing vscode extensions"},{"location":"quickstart/quickstart/#optional-storing-code-inside-wsl","text":"If you are using WSL then it is recommended that you further store your code in WSL 2 filesystem .","title":"Optional: Storing code inside WSL"},{"location":"quickstart/vscode_extensions/","text":"","title":"Vscode extensions"}]}