{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Project Template Welcome to the documentation site for the Data Science Project Template, a codebase that builds containers appropriate for data science projects. While this template provides structure for a Data Science codebase, it is not meant to be prescriptive. For those looking to get up and running quickly , the template has ready integration with docker, vscode and jupyterlab. For those looking to create a reusable code structure in a specific enterprise setting, it might be worthwhile reading the design decisions behind the template. You may want to consider forking this repo as a starting point. Project Status This core functionality of the project and documentation is complete. There are no plans for significant features to be added, but issues raised on GitHub will be addressed. Further Information Quickstart and Installation The Quickstart section section describes how to initialize the template and setup a local development environment. Template Design Information on the template design can be found in the Design section . This section describes the design decisions behind the template, and the structure of the code in more detail. Additional Features The Addition Features subsections describe how the various features of the template can be used in different scenarios. Distribution See the About section for further information on distribution and reuse.","title":"Home"},{"location":"#data-science-project-template","text":"Welcome to the documentation site for the Data Science Project Template, a codebase that builds containers appropriate for data science projects. While this template provides structure for a Data Science codebase, it is not meant to be prescriptive. For those looking to get up and running quickly , the template has ready integration with docker, vscode and jupyterlab. For those looking to create a reusable code structure in a specific enterprise setting, it might be worthwhile reading the design decisions behind the template. You may want to consider forking this repo as a starting point.","title":"Data Science Project Template"},{"location":"#project-status","text":"This core functionality of the project and documentation is complete. There are no plans for significant features to be added, but issues raised on GitHub will be addressed.","title":"Project Status"},{"location":"#further-information","text":"","title":"Further Information"},{"location":"#quickstart-and-installation","text":"The Quickstart section section describes how to initialize the template and setup a local development environment.","title":"Quickstart and Installation"},{"location":"#template-design","text":"Information on the template design can be found in the Design section . This section describes the design decisions behind the template, and the structure of the code in more detail.","title":"Template Design"},{"location":"#additional-features","text":"The Addition Features subsections describe how the various features of the template can be used in different scenarios.","title":"Additional Features"},{"location":"#distribution","text":"See the About section for further information on distribution and reuse.","title":"Distribution"},{"location":"about/about/","text":"About This page contains information about the Data Science Project Template. Author This project was started by Mark Curran , and any issues can be submitted using the github issues system. License This project can be distributed under GNU GENERAL PUBLIC LICENSE Version 3. A link to the full license can be found here .","title":"About"},{"location":"about/about/#about","text":"This page contains information about the Data Science Project Template.","title":"About"},{"location":"about/about/#author","text":"This project was started by Mark Curran , and any issues can be submitted using the github issues system.","title":"Author"},{"location":"about/about/#license","text":"This project can be distributed under GNU GENERAL PUBLIC LICENSE Version 3. A link to the full license can be found here .","title":"License"},{"location":"additional_features/accessing_credentials/","text":"Accessing Credentials This page explains how to access credentials from inside the local container. Git credentials in a local environment If you are using a git credentials helper on the host machine, then VSCode will automatically forward git actions to the credentials helper for authenticating with the remote git repository. Another common scenario is the use of ssh keys to authenticate. In this case, the Remote - Containeters documentation explains how ssh authentication can be made accessible from within the local container. In effect, one has to configure the ssh-agent on the host machine, after which VSCode makes a socket SSH_AUTH_SOCK available from within the container. Credentials from other Providers Accessing credentials on remotely hosted environments depends on how the remote host is configured, and how the provider's client libraries (e.g. python, cli,...) access those credentials. Below are some links to resources that explain how with GCP or AWS credentials are accessed, along with examples for mounting those credentials on a container. In a local environment, setting an environment variable or mounting a read only credentials file is sufficient. On remotely hosted environments, it is recommended to use the provider's language specific SDK to access credentials. GCP This blog post explains mounting the local machine's user credentials onto a container in a local environment. Note, that tor remote environments, Google recommends authenticating using a client library instead of an environment variable in the container. AWS Similarly to GCP, this blog Post explains mounting credentials on a docker container in a local environment. Official AWS advice on credentials for containers running in an AWS remote environment. Like GCP, it's recommended to use a language specific SDK to authenticate. Official AWS deep dive on using service account credentials for containers running on EKS.","title":"Accessing Credentials"},{"location":"additional_features/accessing_credentials/#accessing-credentials","text":"This page explains how to access credentials from inside the local container.","title":"Accessing Credentials"},{"location":"additional_features/accessing_credentials/#git-credentials-in-a-local-environment","text":"If you are using a git credentials helper on the host machine, then VSCode will automatically forward git actions to the credentials helper for authenticating with the remote git repository. Another common scenario is the use of ssh keys to authenticate. In this case, the Remote - Containeters documentation explains how ssh authentication can be made accessible from within the local container. In effect, one has to configure the ssh-agent on the host machine, after which VSCode makes a socket SSH_AUTH_SOCK available from within the container.","title":"Git credentials in a local environment"},{"location":"additional_features/accessing_credentials/#credentials-from-other-providers","text":"Accessing credentials on remotely hosted environments depends on how the remote host is configured, and how the provider's client libraries (e.g. python, cli,...) access those credentials. Below are some links to resources that explain how with GCP or AWS credentials are accessed, along with examples for mounting those credentials on a container. In a local environment, setting an environment variable or mounting a read only credentials file is sufficient. On remotely hosted environments, it is recommended to use the provider's language specific SDK to access credentials.","title":"Credentials from other Providers"},{"location":"additional_features/accessing_credentials/#gcp","text":"This blog post explains mounting the local machine's user credentials onto a container in a local environment. Note, that tor remote environments, Google recommends authenticating using a client library instead of an environment variable in the container.","title":"GCP"},{"location":"additional_features/accessing_credentials/#aws","text":"Similarly to GCP, this blog Post explains mounting credentials on a docker container in a local environment. Official AWS advice on credentials for containers running in an AWS remote environment. Like GCP, it's recommended to use a language specific SDK to authenticate. Official AWS deep dive on using service account credentials for containers running on EKS.","title":"AWS"},{"location":"additional_features/editing_requirements/","text":"Editing a Requirements File The requirements file contains a complete list of all python packages needed for an environment to run. Commands to Edit a Requirements If you would like to add a package to an environment, first install the package using the command pip install <package_name> This will add the package to the evironment's python installation. One can also remove packages using the pip uninstall command. To automatically install these packages next time the container is built, you will need to update the requirements file. To produce the updated package list in the format required, run the command pip freeze and paste its output into the requirements file.","title":"Editing a Requirements File"},{"location":"additional_features/editing_requirements/#editing-a-requirements-file","text":"The requirements file contains a complete list of all python packages needed for an environment to run.","title":"Editing a Requirements File"},{"location":"additional_features/editing_requirements/#commands-to-edit-a-requirements","text":"If you would like to add a package to an environment, first install the package using the command pip install <package_name> This will add the package to the evironment's python installation. One can also remove packages using the pip uninstall command. To automatically install these packages next time the container is built, you will need to update the requirements file. To produce the updated package list in the format required, run the command pip freeze and paste its output into the requirements file.","title":"Commands to Edit a Requirements"},{"location":"additional_features/jupyterlab/","text":"Jupyterlab Jupyterlab is a web-based development environment that is particularly useful for data manipulation and visualization in python. This page explains how to setup Juypyterlab, i.e., the jupyterlab server, in the dev environment. It is strongly recommended to read the section on port mapping below before starting the server. Starting the dev container Start the dev service on the machine that will host the jupyter server docker-compose run -it --service-ports dev /bin/bash Jupyterlab creates notebook files, and it is convenient to run the command to start the server from the folder where you want to store these files. For example, to store your notebooks in dev/notebooks , run this command cd /workspaces/dev/notebooks Note that the root of the repository is /workspaces inside the dev environment. From here start the server by running jupyter lab --port=8001 --ip 0.0.0.0 --no-browser or if you are running as root inside the container jupyter lab --port=8001 --ip 0.0.0.0 --no-browser --allow-root A message should print to the terminal with a URL from which can access the server, for example http://127.0.0.1:8001/lab?token= followed by a long string of characters. These characters are the token that permits access to the server, and by extension the host machine. Do not share the token with people who should not have access to the host machine. You can detach from the container without stopping it by pressing the detach keys sequence for your system (the default is CTRL-p CTRL-q ). Port Mapping By starting the jupyter lab server, you give any user direct access to the host. Therefore, it is important to consider who can access the host's ip address before starting the server. You can specify the host ip address and the port to expose, as explained in the docker-compose specification . Note, that by default docker-compose will not expose ports when running a container , hence when ports are specified in the compose file, then the argument --service-ports must also be passed. Managing extensions and configs It is possible to standardize the behavior of Jupyterlab across a wider team by mounting jupyter configs on the host machine. The jupyter documentation lists common directories and file locations that one could leverage.","title":"Jupyterlab"},{"location":"additional_features/jupyterlab/#jupyterlab","text":"Jupyterlab is a web-based development environment that is particularly useful for data manipulation and visualization in python. This page explains how to setup Juypyterlab, i.e., the jupyterlab server, in the dev environment. It is strongly recommended to read the section on port mapping below before starting the server.","title":"Jupyterlab"},{"location":"additional_features/jupyterlab/#starting-the-dev-container","text":"Start the dev service on the machine that will host the jupyter server docker-compose run -it --service-ports dev /bin/bash Jupyterlab creates notebook files, and it is convenient to run the command to start the server from the folder where you want to store these files. For example, to store your notebooks in dev/notebooks , run this command cd /workspaces/dev/notebooks Note that the root of the repository is /workspaces inside the dev environment. From here start the server by running jupyter lab --port=8001 --ip 0.0.0.0 --no-browser or if you are running as root inside the container jupyter lab --port=8001 --ip 0.0.0.0 --no-browser --allow-root A message should print to the terminal with a URL from which can access the server, for example http://127.0.0.1:8001/lab?token= followed by a long string of characters. These characters are the token that permits access to the server, and by extension the host machine. Do not share the token with people who should not have access to the host machine. You can detach from the container without stopping it by pressing the detach keys sequence for your system (the default is CTRL-p CTRL-q ).","title":"Starting the dev container"},{"location":"additional_features/jupyterlab/#port-mapping","text":"By starting the jupyter lab server, you give any user direct access to the host. Therefore, it is important to consider who can access the host's ip address before starting the server. You can specify the host ip address and the port to expose, as explained in the docker-compose specification . Note, that by default docker-compose will not expose ports when running a container , hence when ports are specified in the compose file, then the argument --service-ports must also be passed.","title":"Port Mapping"},{"location":"additional_features/jupyterlab/#managing-extensions-and-configs","text":"It is possible to standardize the behavior of Jupyterlab across a wider team by mounting jupyter configs on the host machine. The jupyter documentation lists common directories and file locations that one could leverage.","title":"Managing extensions and configs"},{"location":"additional_features/unit_tests/","text":"Unit Tests This page explains how to run unit tests. Unit tests using pytest By default, the template installs pytest into the local environment. This allows one to run any non-resource intensive unit tests on the local host. To run the example unit tests, run this code from the root folder of repository. pytest -v tests/test_regurgitator.py Integration Testing Integration testing is beyond the scope of the Data Science Template, though it is recommended to use a separate environment from dev or prod to run these tests, i.e., a staging environment.","title":"Unit Tests"},{"location":"additional_features/unit_tests/#unit-tests","text":"This page explains how to run unit tests.","title":"Unit Tests"},{"location":"additional_features/unit_tests/#unit-tests-using-pytest","text":"By default, the template installs pytest into the local environment. This allows one to run any non-resource intensive unit tests on the local host. To run the example unit tests, run this code from the root folder of repository. pytest -v tests/test_regurgitator.py","title":"Unit tests using pytest"},{"location":"additional_features/unit_tests/#integration-testing","text":"Integration testing is beyond the scope of the Data Science Template, though it is recommended to use a separate environment from dev or prod to run these tests, i.e., a staging environment.","title":"Integration Testing"},{"location":"additional_features/users_inside_container/","text":"Running Docker Containers as a non-root User This document explains some of the difficulties of using a container as a non-root user, and provides links to resources for changing the default container user. Dangers of running as a non-root user It is recommended that you run docker containers as a non-root user because it could be that the docker daemon is performing operations on the host machine with the same privelages as the container user. If that user is root, then the container can become a conduit for exploiting security flaws in the host system. Changing the Container User Local Environment The Remote - Containers documentation provides some information on using a non-root user to build the container in the local environment. Other Environments It is recommended to add any container users to the Dockerfile . Further information on these commands can be found in the official dockerfile documentation .","title":"Running Docker Containers as a non-root User"},{"location":"additional_features/users_inside_container/#running-docker-containers-as-a-non-root-user","text":"This document explains some of the difficulties of using a container as a non-root user, and provides links to resources for changing the default container user.","title":"Running Docker Containers as a non-root User"},{"location":"additional_features/users_inside_container/#dangers-of-running-as-a-non-root-user","text":"It is recommended that you run docker containers as a non-root user because it could be that the docker daemon is performing operations on the host machine with the same privelages as the container user. If that user is root, then the container can become a conduit for exploiting security flaws in the host system.","title":"Dangers of running as a non-root user"},{"location":"additional_features/users_inside_container/#changing-the-container-user","text":"","title":"Changing the Container User"},{"location":"additional_features/users_inside_container/#local-environment","text":"The Remote - Containers documentation provides some information on using a non-root user to build the container in the local environment.","title":"Local Environment"},{"location":"additional_features/users_inside_container/#other-environments","text":"It is recommended to add any container users to the Dockerfile . Further information on these commands can be found in the official dockerfile documentation .","title":"Other Environments"},{"location":"additional_features/using_mkdocs/","text":"Using mkdocs This page explains how to use mkdocs to build project documentation. mkdocs converts collections of markdown files into a website. The author writes the documentation in a series of markdown files, and writes a single configuration file. In particular, the developer does not have to write the html code of the documentation website. In this page, we explain creating documentation using files that are in the docs/ folder of the main branch of the template repository. The setup of the documentation in this project is not supposed to be prescriptive, but rather to explain a possible documentation solution. Building mkdocs Locally If you are unfamiliar with mkdocs, we recommend reading the official tutorial . To host documentation locally you will need to install the mkdocs python package in the local environment and make sure the appropriate port is forward from your container to the host machine. First, make sure a port is exposed on your container by uncommenting the relevant part of the docker-compose.yml file and rebuilding the container. The choice of port 8000 is arbitrary. To install the mkdocs package, navigate to the docs/ directory and run the command pip install -r docs-requirements.txt To build the documentation website and begin hosting it, run the command: mkdocs serve -a 0.0.0.0:8000 This will build the docs website and host it in port 8000 inside the container . If this port has been forwarded correctly, then opening localhost:8000 in a web browser should load the documentation website. Publishing Docs on GitHub GitHub offers limited amounts of free documentation hosting through github pages . You can manually push to GitHub pages by following the instructions in the mkdocs documentation . You can also setup a GitHub Action to automatically deploy documentation upon merges into a specific branch of the repsoitory. For this project, documentation is deployed upon merges into the main branch. For further information on setting up a GitHub action to deploy documentation, see this post , and the corresponding file in the Data Science Template respository.","title":"Using mkdocs"},{"location":"additional_features/using_mkdocs/#using-mkdocs","text":"This page explains how to use mkdocs to build project documentation. mkdocs converts collections of markdown files into a website. The author writes the documentation in a series of markdown files, and writes a single configuration file. In particular, the developer does not have to write the html code of the documentation website. In this page, we explain creating documentation using files that are in the docs/ folder of the main branch of the template repository. The setup of the documentation in this project is not supposed to be prescriptive, but rather to explain a possible documentation solution.","title":"Using mkdocs"},{"location":"additional_features/using_mkdocs/#building-mkdocs-locally","text":"If you are unfamiliar with mkdocs, we recommend reading the official tutorial . To host documentation locally you will need to install the mkdocs python package in the local environment and make sure the appropriate port is forward from your container to the host machine. First, make sure a port is exposed on your container by uncommenting the relevant part of the docker-compose.yml file and rebuilding the container. The choice of port 8000 is arbitrary. To install the mkdocs package, navigate to the docs/ directory and run the command pip install -r docs-requirements.txt To build the documentation website and begin hosting it, run the command: mkdocs serve -a 0.0.0.0:8000 This will build the docs website and host it in port 8000 inside the container . If this port has been forwarded correctly, then opening localhost:8000 in a web browser should load the documentation website.","title":"Building mkdocs Locally"},{"location":"additional_features/using_mkdocs/#publishing-docs-on-github","text":"GitHub offers limited amounts of free documentation hosting through github pages . You can manually push to GitHub pages by following the instructions in the mkdocs documentation . You can also setup a GitHub Action to automatically deploy documentation upon merges into a specific branch of the repsoitory. For this project, documentation is deployed upon merges into the main branch. For further information on setting up a GitHub action to deploy documentation, see this post , and the corresponding file in the Data Science Template respository.","title":"Publishing Docs on GitHub"},{"location":"additional_features/vscode_extensions/","text":"VSCode Extensions and Settings This page explains how to leverage VSCode extensions and settings. Installing Extensions VSCode extensions add additional features to the editor. In the quickstart guide , we installed the VSCode Remote - Containers extension. Additional extensions will be instaleld automatically when the local contianer is built. To specify which extensions are installed, you can add the extension id to the devcontainer.json (as specified in the devcontain.json reference ) under the propery \"customizations\": \"vscode\": \"extensions\" . You can use the VSCode UI to find an extension's id. Moreover, extensions can be added to the devcontainer.json file through the VSCode UI . In addition, you can specify general VSCode settings under the propery \"customizations\": \"vscode\": \"settings\" . In particular, you can specify extension settings. Extension Locations The final piece of information to be specified is where the extension will run; either in \"ui\" (i.e. the host machine) or \"workspace\"(inside the container). Not all extensions have the flexibility to run in both, and running in the \"ui\" reduces latency. However, if latency is not an issue and the extension supports it, then it's recommended to install the extension in \"workspace\" so that any binaries the extension requires are not installed in the host machine's default environment. To specify a running location edit the property \"remote.extensionKind\" as explained in the official documentation . A more detailed explanation is provided in VSCode API documentation . Installing Binaries In order for an extension to run, it may require the installation of additional packages. Separating out these packages from the packages needed to run the Data Science project itself, is one of the main reasons behind separating local development from dev and prod environments . If additional python packages are required, then they must be added to the local requirements file . See the section on editing the requirements file for more information. Changing Extension Settings Rebuilding the Container After Changing devcontainer.json You can add additional extension settings by creating and editing the file .vscode/settings.json . This file is not git tracked. Moreover, any changes to extension settings in the devcontainer.json file are not enacted until the container is rebuilt. Alternatively, one can install extensions on the local host and mount them on the container . Note Jupyter always installed locally As of September 2022, the Jupyter extension will always be installed when the python extension is. See this GitHub issue for further information. If you would like to uninstall jupyter after building the local container, run the command code --uninstall-extension ms-toolsai.jupyter Note, one cannot rely on using the Lifecycle Scripts to execute this command because these trigger before VSCode begins installing extensions. Data Science Project Template Default Extensions The Data Science Project Template main branch includes the id and settings for several extensions that are useful in the development of a Data Science application, though like most of the template this reflects personal taste and is not supposed to be prescriptive. These extensions are: RunOnSave: The RunOnSave extension coordinates command line actions that are taken upon file save. Different commands are run depending on the file extension. Python: The default python extension provided by Microsoft. The settings file uses mostly default values, with a few exceptions. The main configuration decisions are: pylance is the language server. Problems identified by pylance appear in the PROBLEMS tab in the VSCode UI. pylint and pydocstyle are enabled. Problems identified by these static code checkers appear in the PROBLEMS tab in the VSCode UI. autoDocstring is enabled and provides shortcuts to generate google style docstrings. Upon save, both isort and black are run on all python files. Moreover notifications raised by pylance, pylint and pydocstyle are also refreshed. All code checkers will ignore unresolved imports. This is because heavyweight libraries should be installed in a dev and prod environments , the local environment should be as lightweight as possible. Telemetry is disabled. VSCode UI for unit testing are disabled. Tests can be run from the command line . Rulers are drawn at 79 and 99 characters as a visual guide to help stay within the pep8 recommended line length . Requirements files are automatically sorted . SQLFluff: SQLFluff is run on every file with a sql extension upon file save. Note, that you must specify a dialect in the RunOnSave command. VSCode: Several settings are also imposed on the VSCode editor. These can be overwritten directly in the devcontainer.json file, or by adding .vscode/settings.json file to the local repository. Quick suggestions are disabled. Popups for recommended extensions are disabled. The Cobalt2 theme is the default color theme. Markdown Lint: Markdown files are automatically linted with an indent set to 4 by Markdown Lint . TODO Tree: TODO statements are tracked and highlighted using TODO tree . Links Settings for the VSCode python extension . Settings for the VSCode editor .","title":"VSCode Extensions and Settings"},{"location":"additional_features/vscode_extensions/#vscode-extensions-and-settings","text":"This page explains how to leverage VSCode extensions and settings.","title":"VSCode Extensions and Settings"},{"location":"additional_features/vscode_extensions/#installing-extensions","text":"VSCode extensions add additional features to the editor. In the quickstart guide , we installed the VSCode Remote - Containers extension. Additional extensions will be instaleld automatically when the local contianer is built. To specify which extensions are installed, you can add the extension id to the devcontainer.json (as specified in the devcontain.json reference ) under the propery \"customizations\": \"vscode\": \"extensions\" . You can use the VSCode UI to find an extension's id. Moreover, extensions can be added to the devcontainer.json file through the VSCode UI . In addition, you can specify general VSCode settings under the propery \"customizations\": \"vscode\": \"settings\" . In particular, you can specify extension settings.","title":"Installing Extensions"},{"location":"additional_features/vscode_extensions/#extension-locations","text":"The final piece of information to be specified is where the extension will run; either in \"ui\" (i.e. the host machine) or \"workspace\"(inside the container). Not all extensions have the flexibility to run in both, and running in the \"ui\" reduces latency. However, if latency is not an issue and the extension supports it, then it's recommended to install the extension in \"workspace\" so that any binaries the extension requires are not installed in the host machine's default environment. To specify a running location edit the property \"remote.extensionKind\" as explained in the official documentation . A more detailed explanation is provided in VSCode API documentation .","title":"Extension Locations"},{"location":"additional_features/vscode_extensions/#installing-binaries","text":"In order for an extension to run, it may require the installation of additional packages. Separating out these packages from the packages needed to run the Data Science project itself, is one of the main reasons behind separating local development from dev and prod environments . If additional python packages are required, then they must be added to the local requirements file . See the section on editing the requirements file for more information.","title":"Installing Binaries"},{"location":"additional_features/vscode_extensions/#changing-extension-settings","text":"","title":"Changing Extension Settings"},{"location":"additional_features/vscode_extensions/#rebuilding-the-container-after-changing-devcontainerjson","text":"You can add additional extension settings by creating and editing the file .vscode/settings.json . This file is not git tracked. Moreover, any changes to extension settings in the devcontainer.json file are not enacted until the container is rebuilt. Alternatively, one can install extensions on the local host and mount them on the container .","title":"Rebuilding the Container After Changing devcontainer.json"},{"location":"additional_features/vscode_extensions/#note-jupyter-always-installed-locally","text":"As of September 2022, the Jupyter extension will always be installed when the python extension is. See this GitHub issue for further information. If you would like to uninstall jupyter after building the local container, run the command code --uninstall-extension ms-toolsai.jupyter Note, one cannot rely on using the Lifecycle Scripts to execute this command because these trigger before VSCode begins installing extensions.","title":"Note Jupyter always installed locally"},{"location":"additional_features/vscode_extensions/#data-science-project-template-default-extensions","text":"The Data Science Project Template main branch includes the id and settings for several extensions that are useful in the development of a Data Science application, though like most of the template this reflects personal taste and is not supposed to be prescriptive. These extensions are: RunOnSave: The RunOnSave extension coordinates command line actions that are taken upon file save. Different commands are run depending on the file extension. Python: The default python extension provided by Microsoft. The settings file uses mostly default values, with a few exceptions. The main configuration decisions are: pylance is the language server. Problems identified by pylance appear in the PROBLEMS tab in the VSCode UI. pylint and pydocstyle are enabled. Problems identified by these static code checkers appear in the PROBLEMS tab in the VSCode UI. autoDocstring is enabled and provides shortcuts to generate google style docstrings. Upon save, both isort and black are run on all python files. Moreover notifications raised by pylance, pylint and pydocstyle are also refreshed. All code checkers will ignore unresolved imports. This is because heavyweight libraries should be installed in a dev and prod environments , the local environment should be as lightweight as possible. Telemetry is disabled. VSCode UI for unit testing are disabled. Tests can be run from the command line . Rulers are drawn at 79 and 99 characters as a visual guide to help stay within the pep8 recommended line length . Requirements files are automatically sorted . SQLFluff: SQLFluff is run on every file with a sql extension upon file save. Note, that you must specify a dialect in the RunOnSave command. VSCode: Several settings are also imposed on the VSCode editor. These can be overwritten directly in the devcontainer.json file, or by adding .vscode/settings.json file to the local repository. Quick suggestions are disabled. Popups for recommended extensions are disabled. The Cobalt2 theme is the default color theme. Markdown Lint: Markdown files are automatically linted with an indent set to 4 by Markdown Lint . TODO Tree: TODO statements are tracked and highlighted using TODO tree .","title":"Data Science Project Template Default Extensions"},{"location":"additional_features/vscode_extensions/#links","text":"Settings for the VSCode python extension . Settings for the VSCode editor .","title":"Links"},{"location":"design/design/","text":"Project Design This project was designed to integrate with the environments one encounters in a typical data science project. The definitions of said environments, and whether the default settings of the template address them, is of course open to debate. That said, the template design can be used as a starting point, and is not intended as a one size fits all solution. Environments in a Data Sciene Project Though a data science project can have the same phases (data exploration, model design,...) that one finds in any explanation of a DevOps or MLOps workflow, usually there are three distinct environments one works with. Given the different purposes of these environments, it makes sense to have different configurations to address them. In this project, each environment corresponds to a different docker compose file. Local Development The users local machine, containing source code, git configurations, code linters and checkers. Running elsewhere on the local machine there is probably a service for managing the users personal credentials . A local machine might have network and resource constraints, which makes it inappropriate for large scale data processing. Dev Development A remote environment with access to relevant data. This environment would need access to any relevant data manipulation of ML libraries, e.g., numpy, tensorflow, etc. Packages for debugging, logging and telemetry might be required. Depending on data access permissions, it might make sense to mount the credentials of a service account on this machine. Staging/Production A remote environment identical to production. This environment might be used for testing prior to final deployment or might be production itself. Only the necessary packges are installed in this environment, and personal credentials should certainly not be used to access services. Folder Structure The purpose of each folder in the repository is explained below: .devcontainer/ : All files relating to the local environment. .github/ : Facilitating GitHub actions. app/ : The source code of the application. This is the only folder that should be deployed to prod. dev/ : All files relating to the dev environment. docs/ : For project documentation. tests/ : For testing the contents of app/ folder. Configurations local The local config is stored in the .devcontainer folder . As per the devcontainer specification , the file devcontainer.json configures the local environment by starting the local service in the file docker-compose-local.yml and installing requirements from requirements-local.txt . By default the main branch of the repository contains a number of VSCode extensions that are relevant to local development, including An uncompromising python code formatter . A SQL formatter that works with multiple dialects, in particular, multiple cloud providers. Several python static code checkers . dev The dev config is stored in the dev folder . Start the dev service in docker-compose-dev.yml which installs the requirements requirements-dev.txt . By default the main branch of the repository contains a number of packages that are relevant to dev development. Exploratory data analysis and visualizations using Jupyterlab . Data manipulation using pandas . Further information on using Jupyterlab on remote machine is provided in this page . prod Finally, the prod config is stored in the root folder of the repository . Start the prod service in docker-compose.yml which installs the requirements requirements-prod.txt .\\","title":"Project Design"},{"location":"design/design/#project-design","text":"This project was designed to integrate with the environments one encounters in a typical data science project. The definitions of said environments, and whether the default settings of the template address them, is of course open to debate. That said, the template design can be used as a starting point, and is not intended as a one size fits all solution.","title":"Project Design"},{"location":"design/design/#environments-in-a-data-sciene-project","text":"Though a data science project can have the same phases (data exploration, model design,...) that one finds in any explanation of a DevOps or MLOps workflow, usually there are three distinct environments one works with. Given the different purposes of these environments, it makes sense to have different configurations to address them. In this project, each environment corresponds to a different docker compose file.","title":"Environments in a Data Sciene Project"},{"location":"design/design/#local-development","text":"The users local machine, containing source code, git configurations, code linters and checkers. Running elsewhere on the local machine there is probably a service for managing the users personal credentials . A local machine might have network and resource constraints, which makes it inappropriate for large scale data processing.","title":"Local Development"},{"location":"design/design/#dev-development","text":"A remote environment with access to relevant data. This environment would need access to any relevant data manipulation of ML libraries, e.g., numpy, tensorflow, etc. Packages for debugging, logging and telemetry might be required. Depending on data access permissions, it might make sense to mount the credentials of a service account on this machine.","title":"Dev Development"},{"location":"design/design/#stagingproduction","text":"A remote environment identical to production. This environment might be used for testing prior to final deployment or might be production itself. Only the necessary packges are installed in this environment, and personal credentials should certainly not be used to access services.","title":"Staging/Production"},{"location":"design/design/#folder-structure","text":"The purpose of each folder in the repository is explained below: .devcontainer/ : All files relating to the local environment. .github/ : Facilitating GitHub actions. app/ : The source code of the application. This is the only folder that should be deployed to prod. dev/ : All files relating to the dev environment. docs/ : For project documentation. tests/ : For testing the contents of app/ folder.","title":"Folder Structure"},{"location":"design/design/#configurations","text":"","title":"Configurations"},{"location":"design/design/#local","text":"The local config is stored in the .devcontainer folder . As per the devcontainer specification , the file devcontainer.json configures the local environment by starting the local service in the file docker-compose-local.yml and installing requirements from requirements-local.txt . By default the main branch of the repository contains a number of VSCode extensions that are relevant to local development, including An uncompromising python code formatter . A SQL formatter that works with multiple dialects, in particular, multiple cloud providers. Several python static code checkers .","title":"local"},{"location":"design/design/#dev","text":"The dev config is stored in the dev folder . Start the dev service in docker-compose-dev.yml which installs the requirements requirements-dev.txt . By default the main branch of the repository contains a number of packages that are relevant to dev development. Exploratory data analysis and visualizations using Jupyterlab . Data manipulation using pandas . Further information on using Jupyterlab on remote machine is provided in this page .","title":"dev"},{"location":"design/design/#prod","text":"Finally, the prod config is stored in the root folder of the repository . Start the prod service in docker-compose.yml which installs the requirements requirements-prod.txt .\\","title":"prod"},{"location":"quickstart/quickstart/","text":"Quickstart This page describes how to set up the Data Science Project Template for local development. The project template is in effect a hierarchy of configs that build containers based on the environment that data science is being conducted in. At the bottom of that hierarchy is the local container that runs on a users local machine. This page describes creating and developing inside the local container. Required Software You will need the following applications installed on your local machine. Docker Docker is the application that will run the local container. To install Docker desktop on a Windows, Mac or Linux please to the official site . There are alternatives such as podman , though the project has not been tested on this engine. For Windows Users: Windows Subsystem for Linux If you are using a Windows machine, then you must additionally install Windows Subsystem for Linux . Instructions on how to enable WSL with Docker on a windows machine can be found in the official documentation . Optional: Visual Studio Code Once you have installed Docker, it is highly recommended to install VSCode . The config for the local container is structured according to the devcontainer specification , and VSCode has an extension ( Remote - Containers ) that is compatible with this spec. Once VSCode is installed, install the Remote - Containers extension as per the instructions here . If you are familiar with Docker and are happy to run docker-compose from the command line, then VSCode is not necessary. Optional: Installing Docker into a Linux Subsystem At the time of writing (September 2022), Docker desktop is only available as free software for personal projects and organizations below a certain size. However, the docker engine can be freely installed on any linux machine. To install the docker engine on a linux machine refer to the official documentation which explains how to install the docker engine on various distros, e.g., Ubuntu . If you are running Windows or Mac and your organisation does not have a license to use Docker desktop, you can still run the docker engine from within a virtual machine. Docker Desktop pricing . Running the docker engine in WSL . Running the docker engine on MacOS, both arm64 and x86 chips . Building a Local Development Container Persisting Command Line History The main branch of the Data Science Project Template repository contains all the files you will need to start developing locally, bar one. When developing inside the container, your bash history is kept in the file \"/root/.bash_history\" (by default the container runs as the root user, this page explains some of the limitations of this approach). To prevent your history being destroyed when you rebuild the container, this file is persisted to the file \".devcontainer/.bash_history\" on the local machine. To prevent this file being committed to a remote machine, this file is part of the \".gitignore\", and as such is not present in the main branch. If you would like to persist command line history between sessions, create an empty file \".devcontainer/.bash_history\" before proceeding to build the container for the first time. If you don't want your command line history to be preserved, then remove the mount ./.devcontainer/.bash_history:/root/.bash_history from the file .devcontainer/docker-compose-local.yml . Building and Running the Container To build and connect to the container, open VSCode and open the root folder of the Data Science Project Template repo. Make sure Docker Desktop is running. If the Remote - Containers extension is installed correctly, then you should be able to build the container by bringing up the command palette and entering the command Remote-Containers: Rebuild Container . Installing VSCode extensions By default the main branch of the template has configurations for a number of VSCode extensions in the devcontainer.json file under the property \"customizations\": \"vscode\" : \"extensions\" . These extensions will be installed automatically unless they are removed from devcontainer.json prior to building the container. See the page on VSCode extensions for more information. Optional: Storing code inside WSL If you are using WSL then it is recommended that you further store your code in WSL 2 filesystem .","title":"Quickstart"},{"location":"quickstart/quickstart/#quickstart","text":"This page describes how to set up the Data Science Project Template for local development. The project template is in effect a hierarchy of configs that build containers based on the environment that data science is being conducted in. At the bottom of that hierarchy is the local container that runs on a users local machine. This page describes creating and developing inside the local container.","title":"Quickstart"},{"location":"quickstart/quickstart/#required-software","text":"You will need the following applications installed on your local machine.","title":"Required Software"},{"location":"quickstart/quickstart/#docker","text":"Docker is the application that will run the local container. To install Docker desktop on a Windows, Mac or Linux please to the official site . There are alternatives such as podman , though the project has not been tested on this engine.","title":"Docker"},{"location":"quickstart/quickstart/#for-windows-users-windows-subsystem-for-linux","text":"If you are using a Windows machine, then you must additionally install Windows Subsystem for Linux . Instructions on how to enable WSL with Docker on a windows machine can be found in the official documentation .","title":"For Windows Users: Windows Subsystem for Linux"},{"location":"quickstart/quickstart/#optional-visual-studio-code","text":"Once you have installed Docker, it is highly recommended to install VSCode . The config for the local container is structured according to the devcontainer specification , and VSCode has an extension ( Remote - Containers ) that is compatible with this spec. Once VSCode is installed, install the Remote - Containers extension as per the instructions here . If you are familiar with Docker and are happy to run docker-compose from the command line, then VSCode is not necessary.","title":"Optional: Visual Studio Code"},{"location":"quickstart/quickstart/#optional-installing-docker-into-a-linux-subsystem","text":"At the time of writing (September 2022), Docker desktop is only available as free software for personal projects and organizations below a certain size. However, the docker engine can be freely installed on any linux machine. To install the docker engine on a linux machine refer to the official documentation which explains how to install the docker engine on various distros, e.g., Ubuntu . If you are running Windows or Mac and your organisation does not have a license to use Docker desktop, you can still run the docker engine from within a virtual machine. Docker Desktop pricing . Running the docker engine in WSL . Running the docker engine on MacOS, both arm64 and x86 chips .","title":"Optional: Installing Docker into a Linux Subsystem"},{"location":"quickstart/quickstart/#building-a-local-development-container","text":"","title":"Building a Local Development Container"},{"location":"quickstart/quickstart/#persisting-command-line-history","text":"The main branch of the Data Science Project Template repository contains all the files you will need to start developing locally, bar one. When developing inside the container, your bash history is kept in the file \"/root/.bash_history\" (by default the container runs as the root user, this page explains some of the limitations of this approach). To prevent your history being destroyed when you rebuild the container, this file is persisted to the file \".devcontainer/.bash_history\" on the local machine. To prevent this file being committed to a remote machine, this file is part of the \".gitignore\", and as such is not present in the main branch. If you would like to persist command line history between sessions, create an empty file \".devcontainer/.bash_history\" before proceeding to build the container for the first time. If you don't want your command line history to be preserved, then remove the mount ./.devcontainer/.bash_history:/root/.bash_history from the file .devcontainer/docker-compose-local.yml .","title":"Persisting Command Line History"},{"location":"quickstart/quickstart/#building-and-running-the-container","text":"To build and connect to the container, open VSCode and open the root folder of the Data Science Project Template repo. Make sure Docker Desktop is running. If the Remote - Containers extension is installed correctly, then you should be able to build the container by bringing up the command palette and entering the command Remote-Containers: Rebuild Container .","title":"Building and Running the Container"},{"location":"quickstart/quickstart/#installing-vscode-extensions","text":"By default the main branch of the template has configurations for a number of VSCode extensions in the devcontainer.json file under the property \"customizations\": \"vscode\" : \"extensions\" . These extensions will be installed automatically unless they are removed from devcontainer.json prior to building the container. See the page on VSCode extensions for more information.","title":"Installing VSCode extensions"},{"location":"quickstart/quickstart/#optional-storing-code-inside-wsl","text":"If you are using WSL then it is recommended that you further store your code in WSL 2 filesystem .","title":"Optional: Storing code inside WSL"}]}